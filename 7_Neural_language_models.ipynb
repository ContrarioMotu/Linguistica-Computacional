{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ayala Morales Mauricio\n",
    "### No. de cuenta: 315332122\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j2maL35Lf92i"
   },
   "source": [
    "# Modelos del lenguaje Neuronales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N3wU_JfIgD6s"
   },
   "source": [
    "## Objetivos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "55cvz7yQgFby"
   },
   "source": [
    "- Utilizar una red neuronal para construir un modelo del lenguaje\n",
    "    - Utilizaremos la biblioteca de pytorch\n",
    "    - Generaremos lenguaje\n",
    "- Explorar los embeddings generados por la red"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalación de dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in ./.venv/lib/python3.11/site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.venv/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.11/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.11/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.11/site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./.venv/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./.venv/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./.venv/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.venv/lib/python3.11/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./.venv/lib/python3.11/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./.venv/lib/python3.11/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./.venv/lib/python3.11/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./.venv/lib/python3.11/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./.venv/lib/python3.11/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./.venv/lib/python3.11/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./.venv/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./.venv/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in ./.venv/lib/python3.11/site-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.venv/lib/python3.11/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.11/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.11/site-packages (from jinja2->torch) (2.1.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importación de módulos y bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "SJ0Hn9p7kbXx"
   },
   "outputs": [],
   "source": [
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import ngrams\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cGjSZC6mv2i4"
   },
   "source": [
    "# Práctica 7: Estrategias de generación de texto\n",
    "\n",
    "**Entrega: 10 de Noviembre 2024 11:59p.m.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TFfBxEsMIEDx"
   },
   "source": [
    "- Construir un modelo del lenguaje neuronal a partir de un corpus en español\n",
    "  - Corpus: El Quijote. URL: https://www.gutenberg.org/ebooks/2000\n",
    "    - **NOTA: Considera los recursos de computo. Recuerda que en la practica utilizamos ~50k oraciones**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepocesamiento del corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sent: list[str]) -> list[str]:\n",
    "    \"\"\"\n",
    "    Preprocess the sentence by removing punctuation, converting to lower case, and adding\n",
    "    special tokens of beginning and end of sentence.\n",
    "\n",
    "    :param list[str] sent: List of words in a sentence.\n",
    "    :return list[str]: List of preprocessed words from the sentence.\n",
    "    \"\"\"\n",
    "    result = [word.lower() for word in sent]\n",
    "    result.append(\"<EOS>\")\n",
    "    result.insert(0, \"<BOS>\")\n",
    "    return result\n",
    "\n",
    "with open('ElQuijote_corpus.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "    f.close()\n",
    "\n",
    "corpus = [word_tokenize(sentence) for sentence in text.split('\\n') if sentence.strip() != '']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Modelo de trigramas con `n = 3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_freqs(corpus: list[list[str]]) -> dict:\n",
    "    \"\"\"\n",
    "    Gets the frequency of each word in the corpus.\n",
    "\n",
    "    :param list[list[str]] corpus: list of sentences\n",
    "    :return: dict of words and their frequency\n",
    "    \"\"\"\n",
    "    words_freqs = {}\n",
    "    for sentence in corpus:\n",
    "        for word in sentence:\n",
    "            words_freqs[word] = words_freqs.get(word, 0) + 1\n",
    "    return words_freqs\n",
    "\n",
    "def get_words_indexes(words_freqs: dict) -> tuple[dict, dict]:\n",
    "    \"\"\"\n",
    "    Gets the index of each word in the corpus.\n",
    "\n",
    "    :param dict words_freqs: dict of words and their frequency\n",
    "    :return: dict of words and their index\n",
    "    \"\"\"\n",
    "    UNK_LABEL = \"<UNK>\"\n",
    "    result = {}\n",
    "    for idx, word in enumerate(words_freqs.keys()):\n",
    "        # Happax legomena happends\n",
    "        if words_freqs[word] == 1:\n",
    "            # Temp index for unknowns\n",
    "            result[UNK_LABEL] = len(words_freqs)\n",
    "        else:\n",
    "            result[word] = idx\n",
    "\n",
    "    return ({word: idx for idx, word in enumerate(result.keys())},\n",
    "            {idx: word for idx, word in enumerate(result.keys())})\n",
    "\n",
    "def get_word_id(words_indexes: dict, word: str) -> int:\n",
    "    \"\"\"\n",
    "    Gets the index of a word in the corpus.\n",
    "\n",
    "    :param dict words_indexes: dict of words and their index\n",
    "    :param str word: word to get the index of\n",
    "    :return: index of the word\n",
    "    \"\"\"\n",
    "    UNK_LABEL = \"<UNK>\"\n",
    "    unk_word_id = words_indexes[UNK_LABEL]\n",
    "    return words_indexes.get(word, unk_word_id)\n",
    "\n",
    "def get_train_test_data(corpus: list[list[str]], words_indexes: dict, n: int) -> tuple[list, list]:\n",
    "    \"\"\"\n",
    "    Divides the corpus into train and test data.\n",
    "\n",
    "    :param list[list[str]] corpus: list of sentences\n",
    "    :param dict words_indexes: dict of words and their index\n",
    "    :param int n: n-gram size\n",
    "    :return: tuple of train and test data\n",
    "    \"\"\"\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    for sent in corpus:\n",
    "        n_grams = ngrams(sent, n)\n",
    "        for w1, w2, w3 in n_grams:\n",
    "            x_train.append([get_word_id(words_indexes, w1), get_word_id(words_indexes, w2)])\n",
    "            y_train.append([get_word_id(words_indexes, w3)])\n",
    "    return x_train, y_train\n",
    "\n",
    "words_freqs = get_words_freqs(corpus)\n",
    "\n",
    "words_indexes, index_to_word = get_words_indexes(words_freqs)\n",
    "\n",
    "x_train, y_train = get_train_test_data(corpus, words_indexes, n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Incluye informacion sobre setup de entrenamiento:\n",
    "    - Dimension de embeddings\n",
    "    - Dimsension de capa oculta\n",
    "    - Cantidad de oraciones para entrenamiento\n",
    "    - Batch size y context size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 200\n",
    "H = 100\n",
    "V = len(words_indexes)\n",
    "CONTEXT_SIZE = 2\n",
    "BATCH_SIZE = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividiendo el conjunto de entrenamiento en lotes de 256 elementos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = np.concatenate((x_train, y_train), axis=1)\n",
    "train_loader = DataLoader(train_set, batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementación de la red neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrigramModel(nn.Module):\n",
    "    \"\"\"Clase padre: https://pytorch.org/docs/stable/generated/torch.nn.Module.html\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size, h):\n",
    "        super(TrigramModel, self).__init__()\n",
    "        self.context_size = context_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, h)\n",
    "        self.linear2 = nn.Linear(h, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # x': concatenation of x1 and x2 embeddings   -->\n",
    "        #self.embeddings regresa un vector por cada uno de los índices que se les pase como entrada. view() les cambia el tamaño para concatenarlos\n",
    "        embeds = self.embeddings(inputs).view((-1,self.context_size * self.embedding_dim))\n",
    "        # h: tanh(W_1.x' + b)  -->\n",
    "        out = torch.tanh(self.linear1(embeds))\n",
    "        # W_2.h                 -->\n",
    "        out = self.linear2(out)\n",
    "        # log_softmax(W_2.h)      -->\n",
    "        # dim=1 para que opere sobre renglones, pues al usar batchs tenemos varios vectores de salida\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Pérdida. Negative log-likelihood loss\n",
    "loss_function = nn.NLLLoss()\n",
    "\n",
    "# 2. Instanciar el modelo\n",
    "model = TrigramModel(V, EMBEDDING_DIM, CONTEXT_SIZE, H)\n",
    "\n",
    "# 3. Optimización. ADAM optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr = 2e-3)\n",
    "\n",
    "# ------------------------- TRAIN & SAVE MODEL ------------------------\n",
    "EPOCHS = 10\n",
    "for epoch in range(EPOCHS):\n",
    "    st = time.time()\n",
    "    print(\"\\n--- Training model Epoch: {} ---\".format(epoch))\n",
    "    for it, data_tensor in enumerate(train_loader):\n",
    "        context_tensor = data_tensor[:,0:2]\n",
    "        target_tensor = data_tensor[:,2]\n",
    "\n",
    "        model.zero_grad() #reinicializar los gradientes\n",
    "        #FORWARD:\n",
    "        # get log probabilities over next words\n",
    "        log_probs = model(context_tensor)\n",
    "\n",
    "\n",
    "        # compute loss function\n",
    "        loss = loss_function(log_probs, target_tensor)\n",
    "\n",
    "        #BACKWARD:\n",
    "        # backward pass and update gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if it % 500 == 0:\n",
    "            print(\"Training Iteration {} of epoch {} complete. Loss: {}; Time taken (s): {}\".format(it, epoch, loss.item(), (time.time()-st)))\n",
    "            st = time.time()\n",
    "            #barch_size x len(vocab)\n",
    "\n",
    "    # saving model\n",
    "    model_path = 'model_{}.dat'.format(epoch)\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"Model saved for epoch={epoch} at {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamiento del modelo utilizando GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training model Epoch: 0 ---\n",
      "Training Iteration 0 of epoch 0 complete. Loss: 10.204422950744629; Time taken (s): 0.2035355567932129\n",
      "Training Iteration 500 of epoch 0 complete. Loss: 5.255366802215576; Time taken (s): 6.619696378707886\n",
      "Training Iteration 1000 of epoch 0 complete. Loss: 5.483633041381836; Time taken (s): 6.597528457641602\n",
      "Training Iteration 1500 of epoch 0 complete. Loss: 6.110557556152344; Time taken (s): 6.599625587463379\n",
      "Training Iteration 2000 of epoch 0 complete. Loss: 4.806663990020752; Time taken (s): 6.449951887130737\n",
      "Training Iteration 2500 of epoch 0 complete. Loss: 5.266054630279541; Time taken (s): 6.386515378952026\n",
      "Training Iteration 3000 of epoch 0 complete. Loss: 5.502137184143066; Time taken (s): 6.624919652938843\n",
      "Training Iteration 3500 of epoch 0 complete. Loss: 4.755428314208984; Time taken (s): 6.594188690185547\n",
      "Training Iteration 4000 of epoch 0 complete. Loss: 4.741596221923828; Time taken (s): 6.58011269569397\n",
      "Model saved for epoch=0 at model_gpu_0.dat\n",
      "\n",
      "--- Training model Epoch: 1 ---\n",
      "Training Iteration 0 of epoch 1 complete. Loss: 3.6477279663085938; Time taken (s): 0.01433873176574707\n",
      "Training Iteration 500 of epoch 1 complete. Loss: 4.0038018226623535; Time taken (s): 6.633963346481323\n",
      "Training Iteration 1000 of epoch 1 complete. Loss: 4.213698387145996; Time taken (s): 6.638909816741943\n",
      "Training Iteration 1500 of epoch 1 complete. Loss: 4.426440715789795; Time taken (s): 6.394813776016235\n",
      "Training Iteration 2000 of epoch 1 complete. Loss: 4.073238372802734; Time taken (s): 6.265071630477905\n",
      "Training Iteration 2500 of epoch 1 complete. Loss: 4.228495121002197; Time taken (s): 6.136102914810181\n",
      "Training Iteration 3000 of epoch 1 complete. Loss: 4.376560688018799; Time taken (s): 6.5649096965789795\n",
      "Training Iteration 3500 of epoch 1 complete. Loss: 3.86822247505188; Time taken (s): 6.492203950881958\n",
      "Training Iteration 4000 of epoch 1 complete. Loss: 3.73663592338562; Time taken (s): 6.354861736297607\n",
      "Model saved for epoch=1 at model_gpu_1.dat\n",
      "\n",
      "--- Training model Epoch: 2 ---\n",
      "Training Iteration 0 of epoch 2 complete. Loss: 2.8312668800354004; Time taken (s): 0.014765501022338867\n",
      "Training Iteration 500 of epoch 2 complete. Loss: 3.4902141094207764; Time taken (s): 6.331932544708252\n",
      "Training Iteration 1000 of epoch 2 complete. Loss: 3.58303165435791; Time taken (s): 6.351826190948486\n",
      "Training Iteration 1500 of epoch 2 complete. Loss: 3.6011366844177246; Time taken (s): 6.31721830368042\n",
      "Training Iteration 2000 of epoch 2 complete. Loss: 3.634488582611084; Time taken (s): 6.500343322753906\n",
      "Training Iteration 2500 of epoch 2 complete. Loss: 3.7119803428649902; Time taken (s): 6.616880178451538\n",
      "Training Iteration 3000 of epoch 2 complete. Loss: 3.6744983196258545; Time taken (s): 6.638510227203369\n",
      "Training Iteration 3500 of epoch 2 complete. Loss: 3.397972583770752; Time taken (s): 6.634277105331421\n",
      "Training Iteration 4000 of epoch 2 complete. Loss: 3.2047128677368164; Time taken (s): 6.636940240859985\n",
      "Model saved for epoch=2 at model_gpu_2.dat\n",
      "\n",
      "--- Training model Epoch: 3 ---\n",
      "Training Iteration 0 of epoch 3 complete. Loss: 2.4563169479370117; Time taken (s): 0.014092206954956055\n",
      "Training Iteration 500 of epoch 3 complete. Loss: 3.2740910053253174; Time taken (s): 6.63258695602417\n",
      "Training Iteration 1000 of epoch 3 complete. Loss: 3.2749855518341064; Time taken (s): 6.616483449935913\n",
      "Training Iteration 1500 of epoch 3 complete. Loss: 3.184323310852051; Time taken (s): 6.595456123352051\n",
      "Training Iteration 2000 of epoch 3 complete. Loss: 3.3318982124328613; Time taken (s): 6.478067398071289\n",
      "Training Iteration 2500 of epoch 3 complete. Loss: 3.4233896732330322; Time taken (s): 6.399497032165527\n",
      "Training Iteration 3000 of epoch 3 complete. Loss: 3.2808070182800293; Time taken (s): 6.751217842102051\n",
      "Training Iteration 3500 of epoch 3 complete. Loss: 3.1364142894744873; Time taken (s): 6.762430906295776\n",
      "Training Iteration 4000 of epoch 3 complete. Loss: 2.923917770385742; Time taken (s): 6.737822532653809\n",
      "Model saved for epoch=3 at model_gpu_3.dat\n",
      "\n",
      "--- Training model Epoch: 4 ---\n",
      "Training Iteration 0 of epoch 4 complete. Loss: 2.339078903198242; Time taken (s): 0.014517545700073242\n",
      "Training Iteration 500 of epoch 4 complete. Loss: 3.1352944374084473; Time taken (s): 6.7959654331207275\n",
      "Training Iteration 1000 of epoch 4 complete. Loss: 3.1382529735565186; Time taken (s): 6.775377511978149\n",
      "Training Iteration 1500 of epoch 4 complete. Loss: 2.9896960258483887; Time taken (s): 6.627664804458618\n",
      "Training Iteration 2000 of epoch 4 complete. Loss: 3.1408236026763916; Time taken (s): 6.711449384689331\n",
      "Training Iteration 2500 of epoch 4 complete. Loss: 3.263831615447998; Time taken (s): 6.674166679382324\n",
      "Training Iteration 3000 of epoch 4 complete. Loss: 3.0212349891662598; Time taken (s): 6.739249229431152\n",
      "Training Iteration 3500 of epoch 4 complete. Loss: 3.0003247261047363; Time taken (s): 6.8012189865112305\n",
      "Training Iteration 4000 of epoch 4 complete. Loss: 2.7329158782958984; Time taken (s): 6.849878787994385\n",
      "Model saved for epoch=4 at model_gpu_4.dat\n",
      "\n",
      "--- Training model Epoch: 5 ---\n",
      "Training Iteration 0 of epoch 5 complete. Loss: 2.256816864013672; Time taken (s): 0.01389169692993164\n",
      "Training Iteration 500 of epoch 5 complete. Loss: 3.0307726860046387; Time taken (s): 6.710471868515015\n",
      "Training Iteration 1000 of epoch 5 complete. Loss: 3.0557844638824463; Time taken (s): 6.82361626625061\n",
      "Training Iteration 1500 of epoch 5 complete. Loss: 2.8859217166900635; Time taken (s): 6.8178184032440186\n",
      "Training Iteration 2000 of epoch 5 complete. Loss: 2.9634697437286377; Time taken (s): 6.802275657653809\n",
      "Training Iteration 2500 of epoch 5 complete. Loss: 3.1144630908966064; Time taken (s): 6.818835973739624\n",
      "Training Iteration 3000 of epoch 5 complete. Loss: 2.852134943008423; Time taken (s): 6.850967168807983\n",
      "Training Iteration 3500 of epoch 5 complete. Loss: 2.9192192554473877; Time taken (s): 6.7373740673065186\n",
      "Training Iteration 4000 of epoch 5 complete. Loss: 2.598191499710083; Time taken (s): 6.678277254104614\n",
      "Model saved for epoch=5 at model_gpu_5.dat\n",
      "\n",
      "--- Training model Epoch: 6 ---\n",
      "Training Iteration 0 of epoch 6 complete. Loss: 2.184119701385498; Time taken (s): 0.014441728591918945\n",
      "Training Iteration 500 of epoch 6 complete. Loss: 2.964871406555176; Time taken (s): 6.621954917907715\n",
      "Training Iteration 1000 of epoch 6 complete. Loss: 2.936396837234497; Time taken (s): 6.7377238273620605\n",
      "Training Iteration 1500 of epoch 6 complete. Loss: 2.740947961807251; Time taken (s): 6.825067520141602\n",
      "Training Iteration 2000 of epoch 6 complete. Loss: 2.913910388946533; Time taken (s): 6.783312082290649\n",
      "Training Iteration 2500 of epoch 6 complete. Loss: 3.078958034515381; Time taken (s): 6.81060004234314\n",
      "Training Iteration 3000 of epoch 6 complete. Loss: 2.790602922439575; Time taken (s): 6.692174911499023\n",
      "Training Iteration 3500 of epoch 6 complete. Loss: 2.8487493991851807; Time taken (s): 6.697416543960571\n",
      "Training Iteration 4000 of epoch 6 complete. Loss: 2.508974552154541; Time taken (s): 6.705647945404053\n",
      "Model saved for epoch=6 at model_gpu_6.dat\n",
      "\n",
      "--- Training model Epoch: 7 ---\n",
      "Training Iteration 0 of epoch 7 complete. Loss: 2.137974739074707; Time taken (s): 0.014054059982299805\n",
      "Training Iteration 500 of epoch 7 complete. Loss: 2.9245340824127197; Time taken (s): 6.876668930053711\n",
      "Training Iteration 1000 of epoch 7 complete. Loss: 2.8542795181274414; Time taken (s): 6.826330900192261\n",
      "Training Iteration 1500 of epoch 7 complete. Loss: 2.6771435737609863; Time taken (s): 6.969835042953491\n",
      "Training Iteration 2000 of epoch 7 complete. Loss: 2.856872081756592; Time taken (s): 6.846168518066406\n",
      "Training Iteration 2500 of epoch 7 complete. Loss: 3.012237548828125; Time taken (s): 6.960769414901733\n",
      "Training Iteration 3000 of epoch 7 complete. Loss: 2.720749855041504; Time taken (s): 6.967400789260864\n",
      "Training Iteration 3500 of epoch 7 complete. Loss: 2.801647901535034; Time taken (s): 6.790880918502808\n",
      "Training Iteration 4000 of epoch 7 complete. Loss: 2.4984850883483887; Time taken (s): 6.69533109664917\n",
      "Model saved for epoch=7 at model_gpu_7.dat\n",
      "\n",
      "--- Training model Epoch: 8 ---\n",
      "Training Iteration 0 of epoch 8 complete. Loss: 2.1729657649993896; Time taken (s): 0.014996767044067383\n",
      "Training Iteration 500 of epoch 8 complete. Loss: 2.8848164081573486; Time taken (s): 6.801676034927368\n",
      "Training Iteration 1000 of epoch 8 complete. Loss: 2.8276519775390625; Time taken (s): 6.766937255859375\n",
      "Training Iteration 1500 of epoch 8 complete. Loss: 2.637442111968994; Time taken (s): 6.675011396408081\n",
      "Training Iteration 2000 of epoch 8 complete. Loss: 2.818216323852539; Time taken (s): 6.661733627319336\n",
      "Training Iteration 2500 of epoch 8 complete. Loss: 2.9793498516082764; Time taken (s): 6.646912574768066\n",
      "Training Iteration 3000 of epoch 8 complete. Loss: 2.6517510414123535; Time taken (s): 6.8421900272369385\n",
      "Training Iteration 3500 of epoch 8 complete. Loss: 2.760355234146118; Time taken (s): 6.768543004989624\n",
      "Training Iteration 4000 of epoch 8 complete. Loss: 2.4718687534332275; Time taken (s): 6.789972305297852\n",
      "Model saved for epoch=8 at model_gpu_8.dat\n",
      "\n",
      "--- Training model Epoch: 9 ---\n",
      "Training Iteration 0 of epoch 9 complete. Loss: 2.1283786296844482; Time taken (s): 0.013699531555175781\n",
      "Training Iteration 500 of epoch 9 complete. Loss: 2.8377935886383057; Time taken (s): 6.82218861579895\n",
      "Training Iteration 1000 of epoch 9 complete. Loss: 2.771317481994629; Time taken (s): 6.79332709312439\n",
      "Training Iteration 1500 of epoch 9 complete. Loss: 2.582035541534424; Time taken (s): 6.689177989959717\n",
      "Training Iteration 2000 of epoch 9 complete. Loss: 2.757786512374878; Time taken (s): 6.658189058303833\n",
      "Training Iteration 2500 of epoch 9 complete. Loss: 2.918890953063965; Time taken (s): 6.6706461906433105\n",
      "Training Iteration 3000 of epoch 9 complete. Loss: 2.612532377243042; Time taken (s): 6.756839752197266\n",
      "Training Iteration 3500 of epoch 9 complete. Loss: 2.7262051105499268; Time taken (s): 6.798940420150757\n",
      "Training Iteration 4000 of epoch 9 complete. Loss: 2.4706764221191406; Time taken (s): 6.622098445892334\n",
      "Model saved for epoch=9 at model_gpu_9.dat\n"
     ]
    }
   ],
   "source": [
    "# Seleccionar la GPU si está disponible\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 1. Pérdida. Negative log-likelihood loss\n",
    "loss_function = nn.NLLLoss()\n",
    "\n",
    "# 2. Instanciar el modelo y enviarlo a device\n",
    "model = TrigramModel(V, EMBEDDING_DIM, CONTEXT_SIZE, H).to(device)\n",
    "\n",
    "# 3. Optimización. ADAM optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr = 2e-3)\n",
    "\n",
    "# ------------------------- TRAIN & SAVE MODEL ------------------------\n",
    "EPOCHS = 10\n",
    "for epoch in range(EPOCHS):\n",
    "    st = time.time()\n",
    "    print(\"\\n--- Training model Epoch: {} ---\".format(epoch))\n",
    "    for it, data_tensor in enumerate(train_loader):\n",
    "        # Mover los datos a la GPU\n",
    "        context_tensor = data_tensor[:,0:2].to(device)\n",
    "        target_tensor = data_tensor[:,2].to(device)\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        # FORWARD:\n",
    "        log_probs = model(context_tensor)\n",
    "\n",
    "        # compute loss function\n",
    "        loss = loss_function(log_probs, target_tensor)\n",
    "\n",
    "        # BACKWARD:\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if it % 500 == 0:\n",
    "            print(\"Training Iteration {} of epoch {} complete. Loss: {}; Time taken (s): {}\".format(it, epoch, loss.item(), (time.time()-st)))\n",
    "            st = time.time()\n",
    "\n",
    "    # saving model\n",
    "    model_path = 'model_gpu_{}.dat'.format(epoch)\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"Model saved for epoch={epoch} at {model_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Incluye la liga de drive de tu modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://drive.google.com/file/d/1VZYBN3bcpHDnHM8rqhLMQU2LH6DssTxJ/view?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Imprima en pantalla un tres ejemplos de generacion de texto\n",
    "  - Proponga mejoras en las estrategias de generación de texto vistas en la práctica\n",
    "  - Decriba en que consiste la estrategia propuesta\n",
    "  - Compare la estrategia de la práctica y su propuesta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_366180/3352099299.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_loaded.load_state_dict(torch.load(path))\n"
     ]
    }
   ],
   "source": [
    "PATH = \"model_gpu_9.dat\"\n",
    "\n",
    "def get_model(path: str) -> TrigramModel:\n",
    "    model_loaded = TrigramModel(V, EMBEDDING_DIM, CONTEXT_SIZE, H)\n",
    "    model_loaded.load_state_dict(torch.load(path))\n",
    "    model_loaded.eval()\n",
    "    return model_loaded\n",
    "\n",
    "model = get_model(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKENS = 20\n",
    "TOP_COUNT = 10\n",
    "\n",
    "def get_likely_words(model: TrigramModel, context: str, words_indexes: dict, index_to_word: dict, top_count: int=10) -> list[tuple]:\n",
    "    \"\"\"\n",
    "    Get a list of likely words for a given context.\n",
    "\n",
    "    :param model: Trigram model\n",
    "    :param context: Context to generate text from\n",
    "    :param words_indexes: Indexes of words in the model\n",
    "    :param index_to_word: Words in the model\n",
    "    :param top_count: Number of likely words to return\n",
    "    \"\"\"\n",
    "\n",
    "    model_probs = {}\n",
    "    words = context.split()\n",
    "    idx_word_1 = get_word_id(words_indexes, words[0])\n",
    "    idx_word_2 = get_word_id(words_indexes, words[1])\n",
    "    probs = model(torch.tensor([[idx_word_1, idx_word_2]])).detach().tolist()\n",
    "\n",
    "    for idx, p in enumerate(probs[0]):\n",
    "        model_probs[idx] = p\n",
    "\n",
    "    # Strategy: Sort and get top-K words to generate text\n",
    "    return sorted(((prob, index_to_word[idx]) for idx, prob in model_probs.items()), reverse=True)[:top_count]\n",
    "\n",
    "def get_next_word(words: list[tuple[float, str]]) -> str:\n",
    "    \"\"\"\n",
    "    Get the next word to generate text from the list of likely words.\n",
    "\n",
    "    :param words: List of likely words\n",
    "    :return: Next word to generate text from\n",
    "    \"\"\"\n",
    "    return words[randint(0, len(words)-1)][1]\n",
    "\n",
    "def generate_text(model: TrigramModel, history: str, words_indexes: dict, index_to_word: dict, tokens_count: int=0) -> None:\n",
    "    \"\"\"\n",
    "    Generate text from the given model and history.\n",
    "\n",
    "    :param model: Trigram model\n",
    "    :param history: History of words to generate text from\n",
    "    :param words_indexes: Indexes of words in the model\n",
    "    :param index_to_word: Words in the model\n",
    "    :param tokens_count: Number of tokens generated so far\n",
    "    \"\"\"\n",
    "\n",
    "    next_word = get_next_word(get_likely_words(model, history, words_indexes, index_to_word, top_count=TOP_COUNT))\n",
    "    print(next_word, end=\" \")\n",
    "    tokens_count += 1\n",
    "    if tokens_count == MAX_TOKENS or next_word == \"<EOS>\":\n",
    "        return\n",
    "    generate_text(model, history.split()[1]+ \" \" + next_word, words_indexes, index_to_word, tokens_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BOS> Donde pudieras prosigue los dos no le duelen . Esta doncella del visorrey todas casi tan en la cabeza y el "
     ]
    }
   ],
   "source": [
    "sentence = \"<BOS> Donde\"\n",
    "print(sentence, end=\" \")\n",
    "generate_text(model, sentence, words_indexes, index_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BOS> Pensativo y tenemos mucho gusto no diere cima y se alegraba , que en su castillo que el buitre declara un "
     ]
    }
   ],
   "source": [
    "sentence = \"<BOS> Pensativo\"\n",
    "print(sentence, end=\" \")\n",
    "generate_text(model, sentence, words_indexes, index_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BOS> Confusas estaban sus huesos le puede dar una dueña , que me confiese por las espaldas para algunas cosas , el pastor "
     ]
    }
   ],
   "source": [
    "sentence = \"<BOS> Confusas estaban\"\n",
    "print(sentence, end=\" \")\n",
    "generate_text(model, sentence, words_indexes, index_to_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99b-GgYWxTsW"
   },
   "source": [
    "### Extra\n",
    "\n",
    "- Visualizar en 2D los vectores de las palabras más comunes (excluir STOP WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3VbipB-Wi11Z"
   },
   "source": [
    "## Referencias\n",
    "\n",
    "- [Language models - Lena Voita](https://lena-voita.github.io/nlp_course/language_modeling.html#generation_strategies)\n",
    "- [A Neural Probabilistic Model - Bengio](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)\n",
    "- El código de esta práctica fue fuertemente basado en código de la Dr. Ximea Guitierrez Vasques"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "jupytext": {
   "formats": "ipynb,auto:light"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
