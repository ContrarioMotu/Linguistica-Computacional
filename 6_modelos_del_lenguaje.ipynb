{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ayala Morales Mauricio\n",
    "### No. de cuenta: 315332122\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UndAvhCx62c5"
   },
   "source": [
    "# 6. Modelos del lenguaje"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AO9uvsff3wUe"
   },
   "source": [
    "## Práctica 6: Evaluación de modelos de lenguaje\n",
    "\n",
    "**Fecha de entrega: 03 de Noviembre de 2024**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importación de módulos y bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "id": "nTQ85iSZhQxA"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/mauricio/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import ngrams\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sBdjG6yY4FoA"
   },
   "source": [
    "- Crear un par de modelos del lenguaje usando un **corpus en español**\n",
    "    - Corpus: El Quijote\n",
    "        - URL: https://www.gutenberg.org/ebooks/2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funciones para la creación de los modelos, preprocesamiento de las oraciones, probabilidad de oraciones y cálculo de la perplejidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sent: list[str]) -> list[str]:\n",
    "    \"\"\"\n",
    "    Preprocess the sentence by removing punctuation, converting to lower case, and adding\n",
    "    special tokens of beginning and end of sentence.\n",
    "\n",
    "    :param list[str] sent: List of words in a sentence.\n",
    "    :return list[str]: List of preprocessed words from the sentence.\n",
    "    \"\"\"\n",
    "    result = [word.lower() for word in sent]\n",
    "    result.append(\"<EOS>\")\n",
    "    result.insert(0, \"<BOS>\")\n",
    "    return result\n",
    "\n",
    "def ngram_model(corpus: list[list[str]], n: int) -> defaultdict[str, defaultdict[str, int]]:\n",
    "    \"\"\"\n",
    "    Create a n-gram model from the corpus.\n",
    "\n",
    "    :param list[list[str]] corpus: List of sentences.\n",
    "    :param int n: Degree of the n-gram.\n",
    "    :return defaultdict[str, defaultdict[str, int]]: Dictionary with the frequency of each n-gram.\n",
    "    \"\"\"\n",
    "\n",
    "    model = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "    if n == 1:\n",
    "        for sentence in corpus:\n",
    "            n_grams = ngrams(preprocess(sentence), n)\n",
    "            for w in n_grams:\n",
    "                model[w] += 1\n",
    "        return model\n",
    "\n",
    "    if n == 2:\n",
    "        for sentence in corpus:\n",
    "            n_grams = ngrams(preprocess(sentence), n)\n",
    "            for w1, w2 in n_grams:\n",
    "                model[w1][w2] += 1\n",
    "        return model\n",
    "\n",
    "    for sentence in corpus:\n",
    "        n_grams = ngrams(preprocess(sentence), n)\n",
    "        for t in n_grams:\n",
    "            model[t[:-1]][t[-1]] += 1\n",
    "    return model\n",
    "\n",
    "def calculate_sent_prob(model: defaultdict, sentence: list[str], n: int) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the probability of a sentence given a model.\n",
    "\n",
    "    :param defaultdict[str, defaultdict[str, int]] model: Model of n-grams.\n",
    "    :param str sentence: Sentence.\n",
    "    :param int n: Degree of the n-gram.\n",
    "    :return float: Probability of the sentence.\n",
    "    \"\"\"\n",
    "    n_grams = ngrams(preprocess(sentence), n)\n",
    "    p = 0.0\n",
    "    for gram in n_grams:\n",
    "        if n == 2:\n",
    "            key = gram[0]\n",
    "            value = gram[1]\n",
    "            try:\n",
    "                p += np.log(model[key][value])\n",
    "            except:\n",
    "                p += 0.0\n",
    "            return p\n",
    "        \n",
    "        key = gram[:-1]\n",
    "        value = gram[-1]\n",
    "\n",
    "        try:\n",
    "            p += np.log(model[key][value])\n",
    "        except:\n",
    "            p += 0.0\n",
    "    return p\n",
    "\n",
    "def perplexity(model: defaultdict, corpus: list[list[str]], n: int) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the perplexity of a model on a corpus.\n",
    "\n",
    "    :param defaultdict[str, defaultdict[str, int]] model: Model of n-grams.\n",
    "    :param list[list[str]] corpus: List of sentences.\n",
    "    :param int n: Degree of the n-gram.\n",
    "    :return float: Perplexity of the model.\n",
    "    \"\"\"\n",
    "    pp = []\n",
    "    for sentence in corpus:\n",
    "    #1. Normalizamos y agregamos símbolos especiales:\n",
    "\n",
    "    #Log perplexity calculada para cada oracion:\n",
    "        log_prob = calculate_sent_prob(model, sentence, n)\n",
    "        perplexity = -(log_prob/len(sentence)-1)\n",
    "        pp.append(perplexity)\n",
    "\n",
    "\n",
    "    #promedio de las log perplexity:\n",
    "    return sum(pp) / len(pp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leyendo el archivo con el texto que se usará como corpus y tokenizando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ElQuijote_corpus.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "    f.close()\n",
    "\n",
    "corpus = [word_tokenize(sentence) for sentence in text.split('\\n') if sentence.strip() != '']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Modelo de n-gramas con `n = [2, 3]`\n",
    "    - Hold out con `test = 30%` y `train = 70%`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creando el modelo de Bi-grama y separando en datos de entrenamiento y prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bi-gram train data 67352\n",
      "Bi-gram tests data 28866\n"
     ]
    }
   ],
   "source": [
    "bigram_model = ngram_model(corpus, 2)\n",
    "\n",
    "bi_train_data, bi_test_data = train_test_split(corpus, test_size=0.3)\n",
    "\n",
    "print(\"Bi-gram train data\", len(bi_train_data))\n",
    "print(\"Bi-gram tests data\", len(bi_test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creando el modelo de Tri-grama y separando en datos de entrenamiento y prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tri-gram train data 67352\n",
      "Tri-gram tests data 28866\n"
     ]
    }
   ],
   "source": [
    "trigram_model = ngram_model(corpus, 3)\n",
    "\n",
    "tri_train_data, tri_test_data = train_test_split(corpus, test_size=0.3)\n",
    "\n",
    "print(\"Tri-gram train data\", len(tri_train_data))\n",
    "print(\"Tri-gram tests data\", len(tri_test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Evaluar los modelos y reportar la perplejidad de cada modelo\n",
    "  - Comparar los resultados entre los diferentes modelos del lenguaje (bigramas, trigramas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculando la perplejidad de cada modelo con sus respectivos conjuntos de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram Perplexity: 0.6076382610599397\n",
      "Trigram Perplexity: -0.9249292139711859\n"
     ]
    }
   ],
   "source": [
    "bigram_perplexity = perplexity(bigram_model, bi_test_data, 2)\n",
    "trigram_perplexity = perplexity(trigram_model, tri_test_data, 3)\n",
    "\n",
    "print(\"Bigram Perplexity:\", bigram_perplexity)\n",
    "print(\"Trigram Perplexity:\", trigram_perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ¿Cual fue el modelo mejor evaluado? ¿Porqué?\n",
    "\n",
    "    El modelo con mejor evaluación es el de Tri-gramas, pues obtuvo una menor perplejidad, es decir, un menor nivel de incertidumbre en sus predicciones. Esto se debe a que el n-grama utilizado es de un grado mayor, pues el conocer más palabras anterior nos da más información sobre qué palabras son más probables de aparecer después."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dTAH6joW8IXW"
   },
   "source": [
    "# Referencias\n",
    "\n",
    "- Mucho del código mostrado fue tomado del trabajo de la Dr. Ximena Guitierrez-Vasques\n",
    "- https://lena-voita.github.io/nlp_course/language_modeling.html"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "jupytext": {
   "formats": "ipynb,auto:light"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
